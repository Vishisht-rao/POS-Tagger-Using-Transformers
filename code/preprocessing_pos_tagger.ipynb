{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b94b6d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import conll2000\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59b25869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /home/vishisht/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /home/vishisht/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /home/vishisht/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/vishisht/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')\n",
    "nltk.download('brown')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec321cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank_corpus = list(treebank.tagged_sents(tagset='universal')) # Tagset universal mean universal standard for naming tags\n",
    "brown_corpus = list(brown.tagged_sents(tagset='universal'))\n",
    "conll_corpus = list(conll2000.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8e51189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treebank tags are {'ADV', 'ADJ', 'NUM', 'ADP', 'NOUN', '.', 'DET', 'CONJ', 'X', 'VERB', 'PRON', 'PRT'}\n",
      "brown tags are {'ADV', 'ADJ', 'NUM', 'ADP', 'NOUN', '.', 'DET', 'CONJ', 'X', 'VERB', 'PRON', 'PRT'}\n",
      "conll tags are {'ADV', 'ADJ', 'NUM', 'ADP', 'NOUN', '.', 'DET', 'CONJ', 'X', 'VERB', 'PRON', 'PRT'}\n"
     ]
    }
   ],
   "source": [
    "# For info about the tags https://universaldependencies.org/u/pos/\n",
    "# All unique tags for the datasets\n",
    "# For treebank\n",
    "tree_bank_tags = []\n",
    "for sen in treebank_corpus:\n",
    "    for word in sen:\n",
    "        tree_bank_tags.append(word[1])\n",
    "tree_bank_tags = set(sorted(tree_bank_tags))\n",
    "print(\"Treebank tags are\", tree_bank_tags)\n",
    "\n",
    "\n",
    "# For brown\n",
    "brown_tags = []\n",
    "for sen in brown_corpus:\n",
    "    for word in sen:\n",
    "        brown_tags.append(word[1])\n",
    "brown_tags = set(sorted(brown_tags))\n",
    "print(\"brown tags are\", brown_tags)\n",
    "\n",
    "# For conll2000\n",
    "conll_tags = []\n",
    "for sen in conll_corpus:\n",
    "    for word in sen:\n",
    "        conll_tags.append(word[1])\n",
    "conll_tags = set(sorted(conll_tags))\n",
    "print(\"conll tags are\", conll_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "be430487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72202"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The full corpus\n",
    "final_dataset = treebank_corpus + brown_corpus + conll_corpus\n",
    "# final_dataset = treebank_corpus\n",
    "len(final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8778a9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing '.' with PUNCT\n",
    "#Changing list of tuples to list of lists for editing purposes\n",
    "\n",
    "for sen in range(len(final_dataset)):\n",
    "    for word in range(len(final_dataset[sen])):\n",
    "        final_dataset[sen][word] = list(final_dataset[sen][word])\n",
    "        if final_dataset[sen][word][1] == '.':\n",
    "            final_dataset[sen][word][1] = 'PUNCT'\n",
    "        final_dataset[sen][word] = tuple(final_dataset[sen][word])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8b483ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing Test train Split\n",
    "train_dataset, rem_dataset,  = train_test_split(final_dataset, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0cd95ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset, val_dataset = train_test_split(rem_dataset, train_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ecb2b9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len(train_dataset)\n",
    "test_len = len(test_dataset)\n",
    "val_len = len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "691e54fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57761 7220 7221\n"
     ]
    }
   ],
   "source": [
    "print(train_len, test_len, val_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "90250ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset tags are ['ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'PUNCT', 'VERB', 'X']\n"
     ]
    }
   ],
   "source": [
    "# Extracing all the tags\n",
    "\n",
    "dataset_tags = []\n",
    "for sen in final_dataset:\n",
    "    for word in sen:\n",
    "        dataset_tags.append(word[1])\n",
    "dataset_tags = sorted(set(sorted(dataset_tags)))\n",
    "print(\"Final dataset tags are\", dataset_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c35cea55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'-PAD-': 0, 'ADJ': 1, 'ADP': 2, 'ADV': 3, 'CONJ': 4, 'DET': 5, 'NOUN': 6, 'NUM': 7, 'PRON': 8, 'PRT': 9, 'PUNCT': 10, 'VERB': 11, 'X': 12}\n",
      "\n",
      "{0: '-PAD-', 1: 'ADJ', 2: 'ADP', 3: 'ADV', 4: 'CONJ', 5: 'DET', 6: 'NOUN', 7: 'NUM', 8: 'PRON', 9: 'PRT', 10: 'PUNCT', 11: 'VERB', 12: 'X'}\n"
     ]
    }
   ],
   "source": [
    "# Check about -PAD- later on why it's exactly done\n",
    "# Mapping tag to index for processing later\n",
    "tag_to_index = {}\n",
    "index_to_tag = {}\n",
    "idx = 1\n",
    "tag_to_index['-PAD-'] = 0\n",
    "index_to_tag[0] = '-PAD-'\n",
    "for tag in dataset_tags:\n",
    "    tag_to_index[tag] = idx\n",
    "    index_to_tag[idx] = tag\n",
    "    idx += 1\n",
    "print(tag_to_index)\n",
    "print()\n",
    "print(index_to_tag)\n",
    "n_tags = len(tag_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c6f90813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Sentence Length =  271\n",
      "Average Sentence Length =  21.065442080296393\n"
     ]
    }
   ],
   "source": [
    "# Some statistics\n",
    "max_sen_len = 0\n",
    "avg_sen_len = 0\n",
    "for sen in train_dataset:\n",
    "    max_sen_len = max(max_sen_len, len(sen))\n",
    "    avg_sen_len += len(sen)\n",
    "print(\"Maximum Sentence Length = \", max_sen_len)\n",
    "print(\"Average Sentence Length = \", avg_sen_len/len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ca89f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b9578890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(sentences, max_len):\n",
    "    new=[]\n",
    "    for sen in sentences:\n",
    "        new.append([sen[x:x+max_len] for x in range(0, len(sen), max_len)])\n",
    "    new = [sen for sublist in new for sen in sublist]\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "750706e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = split(train_dataset, MAX_SEQUENCE_LENGTH)\n",
    "test_dataset = split(test_dataset, MAX_SEQUENCE_LENGTH)\n",
    "val_dataset = split(val_dataset, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d6a65cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Sentence Length =  70\n",
      "Average Sentence Length =  20.97899963792479\n"
     ]
    }
   ],
   "source": [
    "# After reducing sentence size\n",
    "max_sen_len = 0\n",
    "avg_sen_len = 0\n",
    "for sen in train_dataset:\n",
    "    max_sen_len = max(max_sen_len, len(sen))\n",
    "    avg_sen_len += len(sen)\n",
    "print(\"Maximum Sentence Length = \", max_sen_len)\n",
    "print(\"Average Sentence Length = \", avg_sen_len/len(train_dataset))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "90d82e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(sentences):\n",
    "    return [[t for w, t in sentence] for sentence in sentences]\n",
    "\n",
    "def get_words(sentences):\n",
    "    return [[w for w, t in sentence] for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8aa0e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = get_words(train_dataset)\n",
    "test_text = get_words(test_dataset)\n",
    "\n",
    "train_tag = get_tags(train_dataset)\n",
    "test_tag = get_tags(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "33a996cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['These', 'my', 'grandmother', 'left', 'in', 'their', 'places', '(', 'they', 'are', 'still', 'there', ',', 'more', 'persistent', 'and', 'longer-lived', 'than', 'the', 'generations', 'of', 'man', ')', 'and', 'planted', 'others', 'like', 'them', ',', 'that', 'flourished', 'without', 'careful', 'tending', '.']\n",
      "['DET', 'DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN', 'PUNCT', 'PRON', 'VERB', 'ADV', 'ADV', 'PUNCT', 'ADV', 'ADJ', 'CONJ', 'ADJ', 'ADP', 'DET', 'NOUN', 'ADP', 'NOUN', 'PUNCT', 'CONJ', 'VERB', 'NOUN', 'ADP', 'PRON', 'PUNCT', 'PRON', 'VERB', 'ADP', 'ADJ', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "print(train_text[0])\n",
    "print(train_tag[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d062593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making it BERT preprocessable input\n",
    "train_text_new = []\n",
    "for sent in train_text:\n",
    "    train_text_new.append(' '.join(sent))\n",
    "    \n",
    "test_text_new = []\n",
    "mx = 0\n",
    "for sent in test_text:\n",
    "    test_text_new.append(' '.join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "91d80c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These my grandmother left in their places ( they are still there , more persistent and longer-lived than the generations of man ) and planted others like them , that flourished without careful tending .\n"
     ]
    }
   ],
   "source": [
    "print(train_text_new[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f15d78d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_labels(labels):\n",
    "    train_label_bert = []\n",
    "    train_label_bert.append('-PAD-')\n",
    "    for i in labels:\n",
    "        train_label_bert.append(i)\n",
    "    train_label_bert.append('-PAD-')\n",
    "    return train_label_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "56ca99de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_padded = []\n",
    "for tags in train_tag:\n",
    "    train_tag_padded.append(bert_labels(tags))\n",
    "    \n",
    "test_tag_padded = []\n",
    "for tags in test_tag:\n",
    "    test_tag_padded.append(bert_labels(tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "70a99e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PAD-', 'DET', 'DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN', 'PUNCT', 'PRON', 'VERB', 'ADV', 'ADV', 'PUNCT', 'ADV', 'ADJ', 'CONJ', 'ADJ', 'ADP', 'DET', 'NOUN', 'ADP', 'NOUN', 'PUNCT', 'CONJ', 'VERB', 'NOUN', 'ADP', 'PRON', 'PUNCT', 'PRON', 'VERB', 'ADP', 'ADJ', 'NOUN', 'PUNCT', '-PAD-']\n"
     ]
    }
   ],
   "source": [
    "print(train_tag_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "83e5484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_ids = []\n",
    "for sen_tag in train_tag_padded:\n",
    "    train_tag_ids.append(np.array([tag_to_index[tag] for tag in sen_tag] + [0]*(MAX_SEQUENCE_LENGTH + 2- len(sen_tag))))\n",
    "\n",
    "train_tag_ids = np.array(train_tag_ids)    \n",
    "\n",
    "test_tag_ids = []\n",
    "for sen_tag in test_tag_padded:\n",
    "    test_tag_ids.append(np.array([tag_to_index[tag] for tag in sen_tag]+ [0]*(MAX_SEQUENCE_LENGTH + 2 - len(sen_tag))))\n",
    "\n",
    "test_tag_ids = np.array(test_tag_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "016cc152",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocessor = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b7ef96f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(bert_preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "718e3608",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_preprocess = bert_preprocess_model(train_text_new)\n",
    "#train_text_preprocess.keys()\n",
    "test_text_preprocess = bert_preprocess_model(test_text_new)\n",
    "#test_text_preprocess.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e412397a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_type_ids', 'input_word_ids', 'input_mask'])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_preprocess.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2fecb51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids = []\n",
    "\n",
    "for input_words_id in train_text_preprocess['input_word_ids']:\n",
    "    train_input_ids.append(np.array(input_words_id[:MAX_SEQUENCE_LENGTH+2]))\n",
    "train_input_ids = np.array(train_input_ids)\n",
    "\n",
    "train_type_ids = []\n",
    "for input_types_id in train_text_preprocess['input_type_ids']:\n",
    "    train_type_ids.append(np.array(input_types_id[:MAX_SEQUENCE_LENGTH+2]))\n",
    "train_type_ids = np.array(train_type_ids)\n",
    "\n",
    "\n",
    "train_input_masks = []\n",
    "for input_masks in train_text_preprocess['input_mask']:\n",
    "    train_input_masks.append(np.array(input_masks[:MAX_SEQUENCE_LENGTH+2]))\n",
    "train_input_masks = np.array(train_input_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d72aca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = []\n",
    "\n",
    "for input_words_id in test_text_preprocess['input_word_ids']:\n",
    "    test_input_ids.append(np.array(input_words_id[:MAX_SEQUENCE_LENGTH+2]))\n",
    "test_input_ids = np.array(test_input_ids)\n",
    "\n",
    "test_type_ids = []\n",
    "for input_types_id in test_text_preprocess['input_type_ids']:\n",
    "    test_type_ids.append(np.array(input_types_id[:MAX_SEQUENCE_LENGTH+2]))\n",
    "test_type_ids = np.array(test_type_ids)\n",
    "\n",
    "\n",
    "test_input_masks = []\n",
    "for input_masks in test_text_preprocess['input_mask']:\n",
    "    test_input_masks.append(np.array(input_masks[:MAX_SEQUENCE_LENGTH+2]))\n",
    "test_input_masks = np.array(test_input_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fe4a29a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "72\n",
      "72\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "print(len(train_input_ids[0]))\n",
    "print(len(train_input_masks[0]))\n",
    "print(len(train_type_ids[0]))\n",
    "print(len(train_tag_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "18a0f3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  2122,  2026,  7133,  2187,  1999,  2037,  3182,  1006,\n",
       "        2027,  2024,  2145,  2045,  1010,  2062, 14516,  1998,  2936,\n",
       "        1011,  2973,  2084,  1996,  8213,  1997,  2158,  1007,  1998,\n",
       "        8461,  2500,  2066,  2068,  1010,  2008, 17893,  2302,  6176,\n",
       "       25069,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "92b05937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d2d259e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_type_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c7b4277e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57999, 72)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tag_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a8cb3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode tags\n",
    "train_tags = to_categorical(train_tag_ids, num_classes=n_tags)\n",
    "test_tags = to_categorical(test_tag_ids, num_classes=n_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4f0f9a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57999, 72, 13)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "97fa8cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3918f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('train_input_ids.npz', train_input_ids)\n",
    "np.savez_compressed('train_input_masks.npz', train_input_masks)\n",
    "np.savez_compressed('train_type_ids.npz', train_type_ids)\n",
    "\n",
    "np.savez_compressed('test_input_ids.npz', test_input_ids)\n",
    "np.savez_compressed('test_input_masks.npz', test_input_masks)\n",
    "np.savez_compressed('test_type_ids.npz', test_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "26bd63fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('train_tags.npz',train_tags)\n",
    "np.savez_compressed('test_tags.npz',test_tags)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
